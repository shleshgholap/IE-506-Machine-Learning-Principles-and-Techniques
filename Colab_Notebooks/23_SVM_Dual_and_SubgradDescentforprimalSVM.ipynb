{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3FCJ5+IYoc41itslFenzu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#SVM Optimization Problem\n","\n","Given a training data set $D=\\{(\\mathbf{x}^i,y^i)\\}_{i=1}^{n}$ of $n$ training samples where ${\\mathbf{x}}^i \\in \\mathcal{X} \\subseteq {\\mathbb{R}}^d$ and $y^i \\in \\mathcal{Y}=\\{+1,-1\\}$, we solve the following optimization problem: \n","\n","$\n","\\begin{align}\n","\\min_{\\mathbf{w}, b, \\xi_i} & \\ \\frac{1}{2} \\|\\mathbf{w}\\|_2^2 + C \\sum_{i=1}^{n} \\xi_i \\\\ \n","\\text{s.t.}  & \\ y^i (\\mathbf{w}^\\top {\\mathbf{x}}^i - b) \\geq 1 - \\xi_i, \\ \\forall i \\in \\{1,2,\\ldots,n\\}, \\\\   \n","& \\ \\xi_i \\geq 0 \\ \\forall i \\in \\{1,2,\\ldots,n\\}. \n","\\end{align}\n","$\n","\n","We note that the SVM optimization problem is a constrained convex optimization problem of the form:\n","\n","$\n","\\begin{align}\n","\\min_{\\theta \\in {\\mathbb{R}}^p} & f(\\theta)  \\\\ \n","\\text{s.t.} \\ g_i(\\theta) & \\leq 0, \\ i=1,2,\\ldots,m, \\\\ \n","\\end{align}\n","$"],"metadata":{"id":"yFQlE94qIJgd"}},{"cell_type":"markdown","source":["$\\large{\\text{Optimality conditions for constrained convex optimization problems}}$\n","\n","Consider the optimization problem (OP1):\n","\n","$\n","\\begin{align}\n","\\min_{\\theta \\in {\\mathbb{R}}^p} & \\ f(\\theta)  \\\\ \n","\\text{s.t.} \\ g_i(\\theta) & \\leq 0, \\ i=1,2,\\ldots,m, \\\\ \n","h_i(\\theta) & = 0, \\ i=1,2,\\ldots,q, \n","\\end{align}\n","$\n","\n","where $f$, $g_i, i = 1,2,\\ldots,m$,  $h_i, i=1,\\ldots,q$ are continuously differentiable convex functions.\n","\n","Then the necessary and sufficient conditions for optimality ${\\color{blue}{\\text{(under an extra qualification condition)}}}$ are given by the following conditions called Karush-Kuhn-Tucker (KKT) conditions. At an optimal point $\\theta^\\star$ the following hold: \n","\n","1. $\\textbf{Stationarity condition}$\n","\n","There exist multipliers (called Lagrange multipliers) $\\lambda_i \\in \\mathbb{R}, i=1,2,\\ldots,m$, such that $\\lambda_i \\geq 0$, and $\\mu_i \\in \\mathbb{R}, i = 1,2,\\ldots,q$, such that:\n","\n","$\n","\\nabla_\\theta f(\\theta^\\star) +  \\sum_{i=1}^{m} \\lambda_i \\nabla_\\theta g_i(\\theta^\\star) + \\sum_{i=1}^{q} \\mu_i \\nabla_\\theta h_i(\\theta^\\star) = \\mathbf{0}.\n","$\n","\n","2. $\\textbf{Primal feasibility conditions}$\n","\n","$\n","\\begin{align}\n","g_i(\\theta) & \\leq 0, \\ i=1,2,\\ldots,m, \\\\ \n","h_i(\\theta) & = 0, \\ i=1,2,\\ldots,q.  \n","\\end{align}\n","$\n","\n","3. $\\textbf{Dual feasibility conditions}$\n","\n","$\\lambda_i \\in \\mathbb{R}, i=1,2,\\ldots,m$, such that $\\lambda_i \\geq 0$, and $\\mu_i \\in \\mathbb{R}, i = 1,2,\\ldots,q$.\n","\n","4. $\\textbf{Complementary slackness condition}$ \n","\n","$\n","\\lambda_i g_i(\\theta^\\star) = 0, \\forall i = 1,2,\\ldots,m.\n","$\n","\n","The use of terms primal and dual will become clear based on the next discussion."],"metadata":{"id":"z2MqamyXKNRL"}},{"cell_type":"markdown","source":["$\\large{\\text{Lagrangian of the primal problem and dual function}}$\n","\n","Typically, the original optimization problem (OP1) is called the primal problem. \n","\n","The ${\\color{red}{\\text{Lagrangian function}}}$ of the primal problem given by:\n","\n","$\n","L(\\theta;\\lambda,\\mu) =  f(\\theta) +  \\sum_{i=1}^{m} \\lambda_i  g_i(\\theta) + \\sum_{i=1}^{q} \\mu_i h_i(\\theta). \n","$\n","\n","The stationarity condition in KKT condition involving the Lagrange multipliers can be understood as having obtained by finding a stationary point of the Lagrange function: \n","\n","$\n","\\inf_{\\theta} L(\\theta; \\lambda, \\mu).\n","$\n","\n","Note that we have used an infimum instead of minimum as minimum might not necessarily exist for the Lagrangian function.  \n","\n","Also note that the infimum $\\inf_{\\theta} L(\\theta; \\lambda, \\mu)$ is a function of the Lagrange multipliers $\\lambda$ and $\\mu$. \n","\n","This function is called the ${\\color{magenta}{\\text{dual function}}}$ and is denoted by $D(\\lambda, \\mu) =   \\inf_{\\theta} L(\\theta; \\lambda, \\mu)$.\n"],"metadata":{"id":"7iCK0EzlTw4h"}},{"cell_type":"markdown","source":["$\\large{\\text{Dual problem}}$\n","\n","The dual problem associated with the primal problem (OP1) is given as:\n","\n","$\n","\\max_{\\lambda, \\mu} D(\\lambda, \\mu) \n","$\n","\n","where $D(\\lambda, \\mu) = \\inf_{\\theta} L(\\theta; \\lambda, \\mu)$.\n","\n","Thus in the dual problem, we maximize the dual function with respect to the dual variables $\\lambda, \\mu$. \n","\n","We will not discuss the idea behind the maximization here.\n","\n","For more discussions, see the reference materials. \n","\n"],"metadata":{"id":"7OOWMWn1VFW-"}},{"cell_type":"markdown","source":["$\\large{\\text{Dual problem of SVM}}$\n","\n","In standard form with inequality constraints of the form $\\leq$, we can write the primal optimization problem of SVM as:\n","\n","$\n","\\begin{align}\n","\\min_{\\mathbf{w}, b, \\xi_i} & \\ \\frac{1}{2} \\|\\mathbf{w}\\|_2^2 + C \\sum_{i=1}^{n} \\xi_i \\\\ \n","\\text{s.t.}  & \\ 1 - \\xi_i - y^i (\\mathbf{w}^\\top {\\mathbf{x}}^i - b) \\leq 0, \\ \\forall i \\in \\{1,2,\\ldots,n\\}, \\\\   \n","& \\ -\\xi_i \\leq 0 \\ \\forall i \\in \\{1,2,\\ldots,n\\}. \n","\\end{align}\n","$\n","\n","\n","From the primal optimization problem of SVM, we can write the Lagrangian function as:\n","\n","$\n","L(\\mathbf{w},b,\\mathbf{\\xi}; \\alpha, \\eta) = \\frac{1}{2} \\|\\mathbf{w}\\|_2^2 + C \\sum_{i=1}^{n} \\xi_i + \\sum_{i=1}^{n} \\alpha_i [1- \\xi_i - y^i(\\mathbf{w}^\\top {\\mathbf{x}}^i - b)] - \\sum_{i=1}^{n} \\eta_i \\xi_i.\n","$\n","\n","Then from the KKT conditions we can write:\n","\n","$\\textbf{Stationarity conditions}$ \n","\n","\n","With respect to $\\mathbf{w}$ we have:\n","\n","$\n","\\begin{align}\n","\\nabla_w L(\\mathbf{w},b,\\mathbf{\\xi}; \\alpha, \\eta)  &= \\mathbf{0} \\\\\n","\\implies  \\mathbf{w} - \\sum_{i=1}^{n} \\alpha_i y^i x^i &= \\mathbf{0} \\\\\n","\\implies  \\mathbf{w} &= \\sum_{i=1}^{n} \\alpha_i y^i x^i.\n","\\end{align}\n","$\n","\n","Note that the weight vector $w$ (the normal to the max-margin separating hyperplane) is determined completely by the linear combination of $y^i x^i$. \n","\n","With respect to $b$ we have:\n","\n","$\n","\\begin{align}\n","\\nabla_b L(\\mathbf{w},b,\\mathbf{\\xi}; \\alpha, \\eta)  &= {0} \\\\\n","\\implies \\sum_{i=1}^{n} \\alpha_i y^i &= 0.\n","\\end{align}\n","$\n","\n","With respect to $\\xi_i, \\forall i \\in \\{1,2,\\ldots,n\\}$ we have:\n","\n","$\n","\\begin{align}\n","\\nabla_{\\xi_i} L(\\mathbf{w},b,\\mathbf{\\xi}; \\alpha, \\eta)  &= {0} \\\\\n","\\implies C - \\alpha_i - \\eta_i &= 0. \n","\\end{align}\n","$\n","\n","$\\textbf{Primal feasibility conditions}$ \n","\n","The constraints in the primal problem (OP1) are retrieved as primal feasibility conditions:\n","\n","$\n","\\begin{align}\n","1- \\xi_i - y^i (\\mathbf{w}^\\top {\\mathbf{x}}^i - b) & \\leq 0, \\ \\forall i \\in \\{1,2,\\ldots,n\\} \\\\   \n","& \\ -\\xi_i \\leq 0 \\ \\forall i \\in \\{1,2,\\ldots,n\\}. \n","\\end{align}\n","$\n","\n","$\\textbf{Dual feasibility conditions}$ \n","\n","$\n","\\alpha_i \\geq 0, \\forall i \\in \\{1,2,\\ldots, n\\}, \\eta_i \\geq 0, \\forall i \\in \\{1,2,\\ldots, n\\}.\n","$\n","\n","$\\textbf{Complementary slackness conditions}$ \n","\n","\n","$\n","\\begin{align}\n","\\alpha_i [1- \\xi_i - y^i(\\mathbf{w}^\\top {\\mathbf{x}}^i - b)] &= 0, \\forall i \\in \\{1,2,\\ldots,n\\},  \\\\\n","\\eta_i \\xi_i &= 0, \\forall i \\in \\{1,2,\\ldots,n\\}.\n","\\end{align}\n","$\n","\n","By substituting the stationarity conditions back into the Lagrangian function, and by using the dual feasibility conditions, we can write the dual function as $\\textbf{(Exercise!)}$:\n","\n","$\n","\\begin{align}\n","D(\\alpha) & = - \\frac{1}{2} \\|\\sum_{i=1}^{n} \\alpha_i y^i x^i \\|_2^2 + \\sum_{i=1}^{n} \\alpha_i \\\\\n","\\text{s.t.} & \\sum_{i=1}^{n} \\alpha_i y^i  = 0, \\\\\n","& 0 \\leq \\alpha_i \\leq C, \\forall i \\in \\{1,2,\\ldots,n\\}.\n","\\end{align}\n","$\n","\n","\n","Hence the dual problem of SVM is given as:\n","\n","$\n","\\begin{align}\n","\\max_\\alpha D(\\alpha) & = - \\frac{1}{2} \\|\\sum_{i=1}^{n} \\alpha_i y^i x^i \\|_2^2 + \\sum_{i=1}^{n} \\alpha_i \\\\\n","\\text{s.t.} & \\sum_{i=1}^{n} \\alpha_i y^i  = 0, \\\\\n","& 0 \\leq \\alpha_i \\leq C, \\forall i \\in \\{1,2,\\ldots,n\\}.\n","\\end{align}\n","$\n"],"metadata":{"id":"ia9vQVFLW2TJ"}},{"cell_type":"markdown","source":["$\\large{\\text{Idea behind support vectors}}$\n","\n","From the complementary slackness conditions, we see that:\n","\n","$\n","\\alpha_i [1- \\xi_i - y^i(\\mathbf{w}^\\top {\\mathbf{x}}^i - b)] = 0, \\forall i \\in \\{1,2,\\ldots,n\\}.\n","$\n","\n","Suppose a data point $x^i$ is correctly classified, then $\\xi_i = 0$ for that point. \n","\n","Now, consider only the correctly classified points.  \n","\n","Recall that $y^i(\\mathbf{w}^\\top {\\mathbf{x}}^i - b)$ denotes the distance of the correctly classified points from the separating hyperplane. \n","\n","Hence when  $y^i(\\mathbf{w}^\\top {\\mathbf{x}}^i - b) = 1$, the point $x^i$ lies on a supporting hyperplane. Recall that the supporting hyperplanes on either side of the separating hyperplane respectively contain only the points belonging to either class $+1$ or $-1$, which are closest to the separating hyperplane on either side.\n","\n","Thus for all points away from the supporting hyperplane we have $y^i(\\mathbf{w}^\\top {\\mathbf{x}}^i - b) \\neq 1$. Hence from the complementary slackness condition, we have $\\alpha_i = 0$ for all $i \\in \\{1,2,\\ldots,n\\}$ such that $x^i$ are away from the supporting hyperplanes. \n","\n","Therefore, the weight vector $w=\\sum_{i=1}^{n} \\alpha_i y^i x^i$ is determined $\\textbf{only}$ using those data points which are exactly on the supporting hyperplane for which $\\alpha_i$ are potentially non-zero. Such points are called ${\\color{orange}{\\text{support vectors}}}$.\n","\n","If we denote the support vector set $\\text{SV} \\subseteq  \\{1,2,\\ldots,n\\}$ then the weight vector can be written as:\n","\n","$\\mathbf{w}=\\sum_{i\\in \\text{SV}} \\alpha_i y^i x^i$.\n"],"metadata":{"id":"LBKmd3K3gzIY"}},{"cell_type":"markdown","source":["$\\large{\\text{A sub-gradient descent procedure to solve SVM optimization problem}}$\n","\n","The SVM optimization problem can be equivalently written as:\n","\n","$\n","\\begin{align}\n","\\min_{\\mathbf{w} \\in {\\mathbb{R}}^d, b\\in {\\mathbb{R}}} F(\\mathbf{w}, b) = & \\ \\frac{1}{2} \\|\\mathbf{w}\\|_2^2 + C \\sum_{i=1}^{n} (\\max\\{0,  1 - y^i (\\mathbf{w}^\\top {\\mathbf{x}}^i - b)\\}).\\\\ \n","\\end{align}\n","$\n","\n","This problem is an unrestricted (or) unconstrained optimization problem in $\\mathbf{w}, b$. Hence we can use a sub-gradient descent procedure for solving the problem:\n","\n","$\\large{\\text{Sub-gradient descent for solving the unconstrained SVM problem}}:$\n","\n","$\n","\\begin{align}\n","&\\textbf{Step 0:}  \\text{Input data set $D$, tolerances $\\epsilon_1, \\epsilon_2$.} \\\\\n","&\\textbf{Step 1:}  \\text{Start with arbitrary $\\mathbf{w}, b$.} \\\\\n","&\\textbf{Step 2:}  \\text{For $k=1,2,\\ldots$} \\\\\n","&\\quad \\quad \\textbf{Step 2.1:} \\text{Compute sub-gradients $\\partial_\\mathbf{w} F$, $\\partial_{b} F$}. \\\\\n","&\\quad \\quad \\textbf{Step 2.2:}  \\text{Compute step length $\\eta$ using line search procedure} \\\\\n","&\\quad \\quad \\textbf{Step 2.3:}  \\mathbf{w} = \\mathbf{w} - \\eta \\partial_\\mathbf{w} F,  b=b-\\eta \\partial_b F\\\\\n","&\\quad \\quad \\textbf{Step 2.4:}  \\text{if $\\|\\partial_{\\mathbf{w},b} F\\|_2 \\leq \\epsilon_1$ break from loop} \\\\\n","&\\quad \\quad \\textbf{Step 2.5:}  \\text{if relative change in function value is $\\leq \\epsilon_2$ break from loop} \\\\\n","&\\textbf{Step 3:}  \\text{ Output $\\mathbf{w},b$}\n","\\end{align}\n","$\n","\n","Note that $\\partial_{\\mathbf{w}} F$ and $\\partial_{b} F$ denote respectively the sub-gradients of the objective function $F$ with respect to $\\mathbf{w}$ and $b$. $\\eta$ denotes the learning rate. \n","\n","\n","$\\textbf{Exercise:}$ Find $\\partial_{\\mathbf{w}} F$ and $\\partial_{b} F$.\n","\n"],"metadata":{"id":"9hMjO-zBIXrD"}},{"cell_type":"markdown","source":["$\\large{\\text{References}}$\n","\n","1. Stephen Boyd and Lieven Vandenberghe. Convex Optimization. : Cambridge University Press, 2004.\n","2. Nello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, 2000."],"metadata":{"id":"T-y8UMpd4h0w"}},{"cell_type":"code","source":[],"metadata":{"id":"XPH9BDqdIW9K"},"execution_count":null,"outputs":[]}]}